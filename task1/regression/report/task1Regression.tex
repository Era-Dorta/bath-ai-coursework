\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,bm}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{array}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}
  
\title{Task 1.2. Supervised Learning:\\ Bayesian Linear Regressor}
\author{Garoe Dorta-Perez\\
CM50246: Machine Learning and AI}
 
\maketitle
 
\section{Introduction}

Simple linear regressors are overconfident in their predictions.
Bayesian linear regressors are an extension of this model used to solve this particular issue.

\section{The problem}

In this model we want to compute a posterior distribution given a set of training samples, as shown in Equation \ref{eq:bayes}. 
Where $\mathbf{w}$ is a one dimensional array with the world state, $\mathbf{X}$ is a matrix with the data points, $\phi$ are the parameters of a linear function of the data.

\begin{equation}
\label{eq:bayes}
Pr(\phi | \mathbf{X}, \mathbf{w} ) = \frac{ Pr(\mathbf{w} | \mathbf{X}, \phi) Pr(\phi)} {Pr(\mathbf{w} | \mathbf{X} )}.
\end{equation}

The prior in Equation \ref{eq:bayes} is a normal distribution with 0 mean and spherical covariance.
And the likelihood is a multivariate normal distribution, as shown in Equation \ref{eq:normMulti}.
Where  $\sigma^2$ is the covariance, $\mathbf{I}$ is the identity matrix and $\boldsymbol{\theta}= \left\{ \phi, \sigma^2 \right\}$.

\begin{equation}
\label{eq:normMulti}
Pr(\mathbf{w} | \mathbf{X}, \boldsymbol{\theta} ) = Norm_{\mathbf{w}}\left[ \mathbf{X}^T \phi, \sigma^2 \mathbf{I} \right].
\end{equation}

The posterior distribution is shown in Equation \ref{eq:posterior}.

\begin{equation}
\begin{split}
\label{eq:posterior}
Pr(\phi | \mathbf{X}, \mathbf{w} ) &= Norm_{\phi}\left[ \frac{1}{\sigma^2} \mathbf{A}^{-1} \mathbf{X} \mathbf{w}, \mathbf{A}^{-1} \right],\\
\mathbf{A} &= \frac{1}{\sigma^2} \mathbf{X} \mathbf{X}^T + \frac{1}{\sigma^2_p} \mathbf{I}.
\end{split}
\end{equation}

The probability distribution of a new world state $w^*$ over new test data $x^*$ is a Normal distribution as shown in Equation \ref{eq:newWorld}.

\begin{equation}
\begin{split}
\label{eq:newWorld}
Pr(w^* | \mathbf{x}^*, \mathbf{X}, \mathbf{w}) &= \int Pr(w^* | \mathbf{x}^*, \phi) Pr(\phi | \mathbf{X}, \mathbf{w} ) d \phi,\\
 &= Norm_{w^*} \left[ \frac{1}{\sigma^2} \mathbf{x}^{*T} \mathbf{A}^{-1} \mathbf{X} \mathbf{w}, \mathbf{x}^{*T} \mathbf{A}^{-1} \mathbf{x}^* + \sigma^2  \right].
\end{split}
\end{equation}

Since our world states $\mathbf{w_i}$ are multivariate, a separate regressor is used for each dimension in  $\mathbf{w_i}$.

\section{Results}

In Table \ref{tab:results} we show results for a font data set.
The Bayesian linear regressors is trained with pictures of \emph{n} in different fonts and their corresponding \emph{m} in the same font.
For testing, \emph{n} with new fonts are used.

\begin{center}
\begin{table}[h]
\caption[Table caption text]{Comparing the reconstrucction of several \emph{m} with different fonts and variances. In blue is the ground truth and in red is the reconstructed character.}
\label{tab:results}
\begin{tabular}{|c|c|c|c|c|}
\hline
 & $\sigma^2=10$ & $\sigma^2=1$ & $\sigma^2=0.01$ & $\sigma^2=0.001$ \\ \hline
 Font1 & 
\includegraphics[scale = 0.2]{images/f1var10} &
\includegraphics[scale = 0.2]{images/f1var1} &
\includegraphics[scale = 0.2]{images/f1var0_1} &
\includegraphics[scale = 0.2]{images/f1var0_01} \\ \hline
Font2 & 
\includegraphics[scale = 0.2]{images/f2var10} &
\includegraphics[scale = 0.2]{images/f2var1} &
\includegraphics[scale = 0.2]{images/f2var0_1} &
\includegraphics[scale = 0.2]{images/f2var0_01} \\ \hline
Font3 & 
\includegraphics[scale = 0.2]{images/f3var10} &
\includegraphics[scale = 0.2]{images/f3var1} &
\includegraphics[scale = 0.2]{images/f3var0_1} &
\includegraphics[scale = 0.2]{images/f3var0_01} \\ \hline
\end{tabular}
\end{table}
\end{center}

As the Table \ref{tab:results} shows smaller variances $\sigma^2$ produce better results.
Therefore, we can conclude that our Prior model of a Normal distribution is a appropriate choice for this data set.
Furthermore, looking at the covariance we can see how confident the prediction.

\end{document}