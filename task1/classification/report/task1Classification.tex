\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,bm}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}
  
\title{Task 1.1. Supervised Learning: Standard Classifier}
\author{Garoe Dorta-Perez\\
CM50246: Machine Learning and AI}
 
\maketitle
 
\section{Introduction}
 
Given some pictures objects and having been asked to classify them in several groups, we are faced with a multi-class classification problem.
One of the options would be to create N one-against-all binary classifiers.
However an approach that naturally handles the multi-class nature of the problem is to use a categorical distribution to model our world. 

\section{The problem}

As stated in the introduction, we are going to fit a Categorical probability model into our data.
Using Bayes' rule we have:

\begin{equation}
\label{bayes}
Pr(\theta | x_{1 \cdots I}) = \frac{\prod_{i = 1}^{I} Pr(\omega = k_{n} | x, \theta) Pr(\theta)} {Pr(x_{1 \cdots I})}\,
\end{equation}

Assuming the data can be classified using linear functions, N activation functions are needed to enforce the constrains.
Since we are solving for multi-class classification a logistic sigmoid function as activation will not be valid.
Therefore a softmax function is used instead for each activation $a_{n}$.

\begin{align}
\label{activations}
a_{n} &= \phi_{n}^{T}x \\
\lambda_{n} = softmax_{n}[a_{1}, a_{2} \cdots a_{N}] &= 
\frac{exp[a_{n}]} {\sum_{m = 1}^{N} exp[a_{m}] }\,
\end{align}

For the Prior we are going to use a Normal distribution with zero mean and $\sigma$ variance.
In order to simplify the calculations we are going to minimise the log of the probability, where $y_{in}$ is the softmax expression for class $n$ and data $i$:

\begin{equation}
\label{L}
L = - log \sum_{i = 1}^{I} y_{in} + \frac{1}{2 \sigma^{2}} \phi^{T} \phi 
\end{equation}

With gradient and Hessian updates being:

\begin{align}
\label{Grad-Hess-Update}
\frac{\delta L}{ \delta \phi_{n}} &= \sum_{i = 1}^{I} \left( y_{in} - \delta \left[ \omega_{i} - n \right] \right) \mathbf{x}_{i}  + \frac{\phi} {\sigma^{2}} \nonumber \\
\frac{\delta^{2} L}{ \delta \phi_{m}\phi_{n}} &= \sum_{i = 1}^{I} y_{im} \left( \delta \left[ m - n \right] - y_{in} \mathbf{x}_{i} \mathbf{x}_{i}^{T}  \right) +  \frac{\delta \left[ m - n \right]}{\sigma^{2}} \nonumber \\
\end{align}

To make the predictions we evaluate a new sample doing a Laplace approximation and then a Monte Carlo integration

\begin{equation}
\label{Predictions}
predictions = \int y_{in} \mathcal{N}_{a} \left( \mu_{a}, \Sigma_{a} \right) da
\end{equation}
 
\section{Results}

We tested the classification algorithm with two different data sets.
The first one consists of hand drawn images of single digits from zero to nine,
stored as 28x28 matrix of pixel values.
While the second one consists of pictures of eight different objects against a blue background, stored as a 576 vector of pixel value.

Below are the prediction accuracy results for different variances for the normal distribution in the prior and for the initial $\phi$ vector. 

\begin{table}[h]
\begin{tabular}{l|lllllll}
Digits data set &  &  &  &  &  &  &  \\ \hline
Prior Variance & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{10} & \multicolumn{1}{l|}{100} & \multicolumn{1}{l|}{1000} & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{1} \\ \hline
Initial $\phi$ & \multicolumn{1}{l|}{0.1} & \multicolumn{1}{l|}{0.1} & \multicolumn{1}{l|}{0.1} & \multicolumn{1}{l|}{0.1} & \multicolumn{1}{l|}{-1} & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{2.5} \\ \hline
Prediction Accuracy & \multicolumn{1}{l|}{88\%} & \multicolumn{1}{l|}{86\%} & \multicolumn{1}{l|}{77\%} & \multicolumn{1}{l|}{48\%} & \multicolumn{1}{l|}{87\%} & \multicolumn{1}{l|}{87\%} & \multicolumn{1}{l|}{83\%} \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\begin{tabular}{l|lllllll}
ETH-80-HoG data set &  &  &  &  &  &  &  \\ \hline
Prior Variance & \multicolumn{1}{l|}{1} & \multicolumn{1}{l|}{10} & \multicolumn{1}{l|}{100} & \multicolumn{1}{l|}{1000} & \multicolumn{1}{l|}{1000} & \multicolumn{1}{l|}{1000} & \multicolumn{1}{l|}{1000} \\ \hline
Initial $\phi$ & \multicolumn{1}{l|}{0.125} & \multicolumn{1}{l|}{0.125} & \multicolumn{1}{l|}{0.125} & \multicolumn{1}{l|}{0.125} & \multicolumn{1}{l|}{-10} & \multicolumn{1}{l|}{-1} & \multicolumn{1}{l|}{10} \\ \hline
Prediction Accuray & \multicolumn{1}{l|}{67\%} & \multicolumn{1}{l|}{74\%} & \multicolumn{1}{l|}{84\%} & \multicolumn{1}{l|}{89\%} & \multicolumn{1}{l|}{89\%} & \multicolumn{1}{l|}{89\%} & \multicolumn{1}{l|}{89\%} \\ \hline
\end{tabular}
\end{table}

For the \textit{Digits} data set initial $\phi$ vector values smaller than -10 and bigger than 5 would give NaN.
Regarding the prior variance it is clear that a smaller value, so decreasing our confidence in the prior, yields better predictions.
Thus indicating that our guess of a normal distribution centred at zero is a not too far fetched. On the other hand, there are not significant changes when using different $\phi$ values. 
Except for 2.5 which gives a decrease in performance, then indicating the existence of a worse local minima in that region.
%I DONT LIKE THIS PHRASE, CHANGE FOR SAMETHING BETTER 

%Check believe spelling
While for the \textit{ETH-80} data set, the trend is better accuracy as the prior variance increases.
Therefore, the the prior believe seems to be mistaken.
While the initial $\phi$ doesn't seem to affect the end result.
However if set to zeros it does not converge or it does it quite slowly.
 
\end{document}