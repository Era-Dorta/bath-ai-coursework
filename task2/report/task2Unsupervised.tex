\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,bm}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{reflection}[2][Reflection]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{proposition}[2][Proposition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\begin{document}
  
\title{Task 2 Unsupervised Learning: \\K-Means and Mixture of Gaussians}
\author{Garoe Dorta-Perez\\
CM50246: Machine Learning and AI}
 
\maketitle
 
\section{Introduction}
 
Unsupervised learning is a challenging task, given that the only information the clustering has is the data itself.
The K-Means technique uses a non probabilistic approach to solve the problem, while the Mixture of Gaussians performs a similar task but using a Bayesian approach.

\section{The problem}

The main objective is to classify our data in different clusters.
We are going to assume that each cluster is linearly separable from the rest.

\subsection{K-Means}

K-Means algorithm is based on clustering according to a set of prototype vectors.
Each prototype will be a representation of a cluster and the data it contains.

The input would be the input matrix data $X$ and the number of cluster $K$.
While the output is a matrix with the cluster means $\boldsymbol{\mu}$ and a one dimensional array $h$ which indexes each point to its cluster.
 
Initially the means of each cluster are assigned by taking $K$ random samples from a multivariate normal distribution with the data mean and covariance.

The algorithm then performs the following actions:

\begin{enumerate}
\item Assign each point to its nearest cluster.
\item Recalculate the means of the clusters.
\end{enumerate}

For the first step, the distance to each cluster is defined as the euclidean distance from the data point to the cluster prototype.
These two stages are repeated until no points get reassigned in one iteration.
 
\subsection{Mixture of Gaussians}

A mixture of Gaussians approach assumes the data can be modelled using a weighted sum of $K$ normal distributions, as shown in Equation \ref{eq:prGaussian}.
Where for each of the normal distributions, $\boldsymbol{\mu}_k$ is the mean, $\mathbf{\Sigma}_k$ is the covariance, $\lambda_k$ is the weight and $\boldsymbol{\theta} = \left\{ \lambda_k, \boldsymbol{\mu}_k, \mathbf{\Sigma}_k \right\}_{k=1}^K $.

\begin{equation}
\begin{split}
\label{eq:prGaussian}
Pr(\mathbf{x}|\boldsymbol{\theta}) &= \sum_{k=1}^K \lambda_k Norm_x \left[ \boldsymbol{\mu}_k, \mathbf{\Sigma}_k \right],\\
s.t. \qquad & \sum_{k = 1}^K \lambda_k = 1.
\end{split}
\end{equation}

%Why noooot????
A closed form solution is not found if we try to learn the parameters using a maximum likelihood approach.
One alternative is to use an expectation maximization technique.
Before we can apply it, a hidden variable $h \in \left\{ 1 \cdots K \right\} $ has to be added to the model, as shown in Equation \ref{eq:addHidden}.

\begin{equation}
\begin{split}
\label{eq:addHidden}
Pr(\mathbf{x}| h, \boldsymbol{\theta} ) &= Norm_{\mathbf{x}} \left[ \boldsymbol{\mu}_h, \mathbf{\Sigma}_h \right],\\
Pr(h | \boldsymbol{\theta}) &= Cat_h \left[ \boldsymbol{\lambda} \right],\\
Pr(\mathbf{x}| \boldsymbol{\theta}) &= \sum_{k=1}^K Pr(\mathbf{x}, h = k, \boldsymbol{\theta}).
\end{split}
\end{equation}


An expectation maximization consist of a initialization, followed by alternating E-steps and M-steps until a solution is found.
The initialization step is shown in Equation \ref{eq:mixInit}, where $Rand()$ is a random value and $\Sigma_{\mathbf{X}}$ is the overall covariance of the data set.

\begin{align}
\begin{split}
\label{eq:mixInit}
\lambda_k^0 &= \frac{1}{K},\\
\mu_k^0 &= Rand(),\\
\Sigma_k^0 &= \Sigma_{\mathbf{X}}.
\end{split}
\end{align}

For the E-step the posterior probability distribution $Pr(h_i|\mathbf{x}_i)$ of each hidden variable $h_i$ is calculated, as shown in Equation \ref{eq:respCalc}.

\begin{equation}
\label{eq:respCalc}
Pr(h_i = k|\mathbf{x}_i, \boldsymbol{\theta}^{[t]}) = \frac{\lambda_k Norm_{\mathbf{x}_i} \left[ \boldsymbol{\mu}_k, \mathbf{\Sigma}_k \right] } { \sum_{j=1}^K \lambda_k Norm_{\mathbf{x}_i} \left[ \boldsymbol{\mu}_j, \mathbf{\Sigma}_j \right] } = r_{ik}.
\end{equation}

In the M-step the $\lambda$, $\boldsymbol\mu$ and $\mathbf{\Sigma}$ are updated, as shown in Equation \ref{eq:lmuUpd}.

\begin{equation}
\begin{split}
\label{eq:lmuUpd}
\lambda_k^{[t+1]} &= \frac{\sum_{i=1}^I r_{ik}} {\sum_{j=1}^K \sum_{i=1}^I r_{ij}},\\
\boldsymbol \mu_k^{t+1} &= \frac{\sum_{i=1}^I r_{ik} \mathbf{x}_i}{\sum_{i=1}^Ir_{ik}},\\
\mathbf{\Sigma_k^{t+1}} &= \frac{\sum_{i=1}^I r_{ik} ( \mathbf{x}_i - \boldsymbol{\mu}_k^{[t+1]} ) ( \mathbf{x}_i - \boldsymbol{\mu}_k^{[t+1]} )^T } {\sum_{i=1}^Ir_{ik}}.
\end{split}
\end{equation}

An alternative to the random initialization for the means of the Gaussians is to run the k-means first and then assign the cluster means to the Gaussians means.
I.e. $\mu^0_k = \mu_{mi}$, where $\mu_{mi}$ is the mean of the cluster $i$ in the k-means output. 

\section{Results}

K-Means performance varies greatly due to its random initialization, suffering greatly from local minima, as shown in Table \ref{tab:results}.
To obtain the data, the training set \emph{MNIST\_Data} was used.
Each cluster or Gaussian was designated as representative of a digit, we compute how many digits of each class were assigned to the current cluster, and we take $max \sum_{l=1}^{L} x_{l}$ 

Using the Gaussian mixture model with random initialization yields worse results than the k-means.
However, this is probably due to the random initialization, as they should give on average similar results.
When using the kmeans initialization the hits rate increases slightly, so we have a probabilistic classifier with similar accuracy as a non probabilistic one.
Lastly, the relatively low hit rates may imply that the data is not linearly separable.

\begin{table}[h]
\caption[Table caption text]{Percentages of correctly classified digits, showing data for different models and random initializations, where $rs$ is the value of the random seed initialization in the matlab $rng$ function }
\label{tab:results}
\begin{tabular}{|l|c|c|c|}
\hline
Model                 & $rs = ` default \textrm'$ & $rs = 1$ & $rs = 10$ \\ \hline
K-Means               & 53.73\%             &   	59.30\%    &    60.73\%     \\ \hline
Mixture of  Gaussians & 38.13\%             &   	62.37\%    &    46.70\%     \\ \hline
Combined              & 54.03\%             &    61.50\%    &    62.93\%     \\ \hline
\end{tabular}
\end{table}
 
\end{document}